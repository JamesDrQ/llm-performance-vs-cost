{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline.   This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "**Scoring:** Out of $10$ points\n",
    "\n",
    "- Each Developing => $-1$ pts\n",
    "- Each Unsatisfactory/Missing => $-2$ pts\n",
    "\t- until the score is 0\n",
    "\n",
    "If students address the detailed feedback in a future checkpoint, they will **earn these points back**.\n",
    "\n",
    "\n",
    "|                   | Unsatisfactory                                                                                                                                                                                                                                                                                    | Developing                                                                                                                                                                                                                                                                                                                                   | Proficient                                                                                                                                                                                                                                                    | Excellent                                                                                                                                                                                                                                                                                                                      |\n",
    "|-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Research question | The research issue remains unclear. The research purpose, questions, hypotheses, definitions variables, and controls are still largely undefined, or when they are poorly formed, ambiguous, or not logically connected to the description of the problem. Unclear connections to the literature. | The research issue is identified, but the statement is too broad or fails to establish the importance of the problem. The research purpose, questions, hypotheses, definitions or variables, and controls are poorly formed, ambiguous, or not logically connected to the description of the problem. Unclear connections to the literature. | Identifies a relevant research issue. Research questions are succinctly stated, connected to the research issue, and supported by the literature. Variables and controls have been identified and described. Connections are established with the literature. | Presents a significant research problem. Articulates clear, reasonable research questions given the purpose, design, and methods of the project. All variables and controls have been appropriately defined. Proposals are clearly supported by the research and theoretical literature. All elements are mutually supportive. |\n",
    "| Background        | Did not have at least 2 reliable and relevant sources. Or relevant sources were not used in relevant ways                                                                                                                                                                                         | A key component was not connected to the research literature. Selected literature was from unreliable sources. Literary supports were vague or ambiguous.                                                                                                                                                                                    | Key research components were connected to relevant, reliable theoretical and research literature.                                                                                                                                                             | The narrative integrates critical and logical details from the peer-reviewed theoretical and research literature. Each key research component is grounded in the literature. Attention is given to different perspectives, threats to validity, and opinion vs. evidence.                                                      |\n",
    "| Hypothesis        | Lacks most details; vague or interpretable in different ways. Or seems completely unrealistic.                                                                                                                                                                                                    | A key detail to understand the hypothesis or the rationale behind it was not described well enough                                                                                                                                                                                                                                           | The hypothesis is clear. All elements needed to understand the rationale were described in sufficient detail                                                                                                                                                  | The hypothesis and its rationale were described succinctly and with clarity about how they are connected to each other                                                                                                                                                                                                         |\n",
    "| Data              | Did not describe ideal dataset fully AND does not include at least one reference to an external source of data.                                                                                                                                                                         | Either does not describe the ideal dataset fully AND does not include at least one reference to an external source of data that could be used to answer the proposed question.                                                                                                                                                                                                                                                            | Ideal dataset(s) well-described and includes everything needed for answering question(s) posed. Includes at least one reference to a source of data that would be needed to fully answer the question proposed.                                                                                                                                                            | Ideal dataset(s) well-described and includes everything needed for answering question(s) posed. Includes references to all sources of data that would be needed to fully answer the question proposed. The details of the descriptions also make it clear how they support the needs of the project and discuss the differences betweeen the ideal and real datasets.                                                                                                                       |\n",
    "| Ethics            | No effort or just says we have no ethical concerns                                                                                                                                                                                                                                                | Minimal ethical section; probably just talks about data privacy and no unintended consequences discussion. Ethical concerns raised seem irrelevant.                                                                                                                                                                                          | The ethical concerns described are appropriate and sufficiently                                                                                                                                                                                               | Ethical concerns are described clearly and succinctly. This was clearly a thorough and nuanced approach to the issues                                                                                                                                                                                                          |\n",
    "| Team expectations | Lack of expectations                                                                                                                                                                                                                                                                              | The list of expectations feels incomplete and perfunctory                                                                                                                                                                                                                                                                                    | It feels like the list of expectations is complete and seems appropriate                                                                                                                                                                                      | The list clearly was the subject of a thoughtful approach and already indicates a well-working team                                                                                                                                                                                                                            |\n",
    "| Timeline          | Lack of timeline. Or timeline is completely unrealistic                                                                                                                                                                                                                                           | The timeline feels incomplete and perfunctory. The timeline feels either too fast or too slow for the progress you expect a group can make                                                                                                                                                                                                   | It feels like the timeline is complete and appropriate. it can likely be completed as is in the available amount of time                                                                                                                                      | The timeline was clearly the subject of a thoughtful approach and indicates that the team has a detailed plan that seems appropriate and completeable in the allotted time.                                                                                                                                                    |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Project Proposal\n",
    "\n",
    "## Authors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
    "\n",
    "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "Example team list and credits:\n",
    "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
    "- Bob Barker:  Analysis, Software, Visualization\n",
    "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
    "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------(finished)------------------------\n",
    "- James Qian:\n",
    "- Even Wu:\n",
    "- Matthew Odom:\n",
    "- Aston Martini-Facio:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project investigates the relationship between large language model (LLM) performance and token cost. Specifically, it asks: Which publicly available AI models achieve the highest overall benchmark performance, and is that performance proportional to their token costs? To answer this, we will compare models ranked at the top of public leaderboards across multiple evaluation domains (reasoning, coding, math, etc.) with their published pricing structures. We will then fit a predictive model using benchmark scores as independent variables and token cost as the dependent variable, in order to test whether performance can reliably explain or predict pricing.\n",
    "\n",
    "Subquestions might include: \n",
    "\n",
    "- Which models deliver the best overall performance across benchmarks?\n",
    "\n",
    "- Which models provide the most cost-efficient performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models (LLMs) are typically evaluated using standardized benchmarks that measure reasoning, coding, mathematics, and general knowledge. Widely used tasks include MMLU for academic knowledge, GSM8K for multi-step math reasoning, and HumanEval for code generation. These benchmarks provide us with a quantitative basis for comparing model performances, giving us a an accuracy score that can be standarized across different models. Besides static benchmarks, there are also preference-based evaluations such as the LMSYS Chatbot Arena that uses large-scale human comparisons to capture perceived quality across diverse prompts, offering a more holistic view of model performance.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1)\n",
    "\n",
    "Alongside performance data, providers publish detailed token pricing, typically expressed as cost per million input and output tokens. OpenAI, Google and any other providers all differentiate their models by capability tier, context window, and modality, creating a transparent but complex pricing structure.<a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3) While pricing is public, it is not always clear whether higher benchmark performance directly explains higher costs. Some analyses suggest that cost-efficiency varies significantly across models, with smaller or open-source systems sometimes offering better performance-per-dollar on specific tasks.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2)\n",
    "\n",
    "A simple google search reveals this statement insight on 2 top models “ChatGpt is better for general users seeking feature rich experience while deepseek is superior in technical tasks…” (google 2025).  Testing artificial intelligence on a variety of categories can result in metrics which can be used as a foundation for determining what is “best”. Topics which worked in the past to help analyse artificial intelligence are: model, operationalization, proxies, ground truth, bias, machine learning, power, discrimination, agency, privacy and harms, data brokers, consent, goodhearts law, accountability, and standpoint epistemology. Leveraging this vocabulary we can begin to talk about something intangible and circumstantial like “quality” of Ai. Other topics to consider are speed, coding success, accuracy, creativity, multimodality, user experience, and cost. For example if chatGPT is functionally superior, but steals all your data, personal info and even steals your current project that you used chatGPT for, is it “better”?\n",
    "\n",
    "*Past work examples:*\n",
    "\n",
    "In this project<a name=\"cite_ref-5\"></a>[<sup>5</sup>](#cite_note-5) the authors are attempting to experiment with a similar question. “who wins the battle of wits?” chatGPT or deepseek. This author has done something unique, they have made the model itself do most of the actual works. As illustrated by this example, what to name the article. \n",
    "chatGPT : GPT vs. DeepSeek: Which AI Wins for Technical Writing?\n",
    "deepseek: GPT vs Deepseek: Which AI is Your Ultimate Wingman for Technical Writing?\n",
    "-which is better-\n",
    "Through the test making the model to reveal itself by itself doing the test is a clever inception method to run this project. Do we somehow trick the Ai to be tested on testing itself? to devise this test itself may require much planning. A traditional method may result in more clear results.\n",
    "\n",
    "Another example is the ongoing test of the top Ai models in the world of financial trading. started october 17, 2025 the top Ai models are each given 10,000 dollars to practice stock trading. Since then chatGPT has lowered to the bottom group and deepseek has risen to the top.<a name=\"cite_ref-4\"></a>[<sup>4</sup>](#cite_note-4) Since we are seeking information on the quality of the available Ai models on the market today. We will observe the model's token metrics in cross reference with the quality of benchmarks performance tests just like these to determine the result.\n",
    "\n",
    "Prior works have highlighted both the strengths and limitations of current evaluation practices. The Hugging Face Open LLM Leaderboard aggregates multiple benchmarks into composite rankings, displaying trade-offs between accuracy and efficiency.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) Surveys of evaluation methods emphasize the need for multidimensional comparisons, noting that no single benchmark captures the full spectrum of model capabilities.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1)Building on these foundations, our project will investigate whether benchmark performance in terms of accuracy score can reliably predict token pricing.\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1)Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, Ion Stoica. \"LMSYS Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings\", 03 May, 2023. https://lmsys.org/blog/2023-05-03-arena/\n",
    "\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2)Hugging Face Open LLM Leaderboard. \"Leaderboards on the Hub\" https://huggingface.co/docs/leaderboards/en/leaderboards/intro\n",
    "\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3)OpenAI API pricing. https://openai.com/api/pricing\n",
    "\n",
    "4. <a name=\"cite_note-4\"></a> [^](#cite_ref-4)“Ai Trading Benchmark.” Alpha Arena, Accessed 29 Oct. 2025. https://nof1.ai/\n",
    "\n",
    "5. <a name=\"cite_note-5\"></a> [^](#cite_ref-5)“Deepseek vs Chatgpt: Which Ai Is Right for You?” InvoZone, 19 Oct, 2025. https://invozone.com/blog/deepseek-vs-chatgpt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null Hypothesis:** There is no significant relationship between a LLM model’s benchmark accuracy score and its token cost. Improvements in performance do not systematically result in higher or lower token cost.\n",
    "- Benchmark scores and pricing are shaped by different forces. As noted in the background, public pricing is transparent but complex, and smaller or open models can be more cost-efficient on specific tasks—suggesting performance alone may not systematically dictate token cost.\n",
    "\n",
    "**Alternative Hypothesis:** A LLM model's benchmark accuracy is significantly related to token cost — either positively (higher performance requires more cost) or non-linearly (diminishing returns).\n",
    "- Providers often price higher-capability tiers above lower ones, and aggregate benchmarks tend to correlate with perceived quality, which can command premium pricing. However, we also expect non-linearity: gains at the top end may incur disproportionately higher costs due to scaling, yielding diminishing returns.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We are investigating the relationship between LLM model performance and the individual token cost. Therefore, we need information about the evaluation on LLM models, better be comprehensive and include many criterions, and the token costs, which is relatively simpler.\n",
    "   Variables are: LLM model names, performance scores (math, coding, literature, image recognition, etc), weighted score, token cost. We would collect these data from datasets online.\n",
    "   We need as much obervations as possible, to eliminate potential bias.\n",
    "   James edits this part, and will look for suitable datasets. Try to find these datasets on websites like kaggle, huggingface, etc. These data would be stored and organized on github.\n",
    "\n",
    "3. For example, https://www.vellum.ai/open-llm-leaderboard?utm_source=www.vellum.ai&utm_medium=referral provides a decent \"open model comparison.\" It has different LLM models and their performances in tests, such as the math competition AIME. It's a small dataset, so we will also look for other bigger datasets.\n",
    "   https://nof1.ai/ is an interesting performance criterion, about LLM models' ability to make money. This is just an example, but we want detailed data about AI's performances like this.\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics \n",
    "\n",
    "Instructions: Keep the contents of this cell. For each item on the checklist\n",
    "-  put an X there if you've considered the item\n",
    "-  IF THE ITEM IS RELEVANT place a short paragraph after the checklist item discussing the issue.\n",
    "  \n",
    "Items on this checklist are meant to provoke discussion among good-faith actors who take their ethical responsibilities seriously. Your teams will document these discussions and decisions for posterity using this section.  You don't have to solve these problems, you just have to acknowledge any potential harm no matter how unlikely.\n",
    "\n",
    "Here is a [list of real world examples](https://deon.drivendata.org/examples/) for each item in the checklist that can refer to.\n",
    "\n",
    "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n",
    "\n",
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    "> Example of how to use the checkbox, and also of how you can put in a short paragraph that discusses the way this checklist item affects your project.  Remove this paragraph and the X in the checkbox before you fill this out for your project\n",
    "\n",
    " - [ ] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    " - [ ] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    " - [ ] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "### B. Data Storage\n",
    " - [ ] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    " - [ ] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [ ] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "### C. Analysis\n",
    " - [ ] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    " - [ ] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    " - [ ] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    " - [ ] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    " - [ ] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "### D. Modeling\n",
    " - [ ] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    " - [ ] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [ ] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    " - [ ] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    " - [ ] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "### E. Deployment\n",
    " - [ ] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [ ] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    " - [ ] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [ ] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work\n",
    "  \n",
    "Read over the [COGS108 Team Policies](https://github.com/COGS108/Projects/blob/master/COGS108_TeamPolicies.md) individually. Then, include your group’s expectations of one another for successful completion of your COGS108 project below. Discuss and agree on what all of your expectations are. Discuss how your team will communicate throughout the quarter and consider how you will communicate respectfully should conflicts arise. By including each member’s name above and by adding their name to the submission, you are indicating that you have read the COGS108 Team Policies, accept your team’s expectations below, and have every intention to fulfill them. These expectations are for your team’s use and benefit — they won’t be graded for their details.\n",
    "\n",
    "* *Team Expectation 1*\n",
    "* *Team Expectation 2*\n",
    "* *Team Expecation 3*\n",
    "* ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work\n",
    "\n",
    "Specify your team's specific project timeline. An example timeline has been provided. Changes the dates, times, names, and details to fit your group's plan.\n",
    "\n",
    "If you think you will need any special resources or training outside what we have covered in COGS 108 to solve your problem, then your proposal should state these clearly. For example, if you have selected a problem that involves implementing multiple neural networks, please state this so we can make sure you know what you’re doing and so we can point you to resources you will need to implement your project. Note that you are not required to use outside methods.\n",
    "\n",
    "\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM | Read & Think about COGS 108 expectations; brainstorm topics/questions  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data (Ant Man); EDA (Hulk) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin Analysis (Iron Man; Thor) | Discuss/edit Analysis; Complete project check-in |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Wasp)| Discuss/edit full project |\n",
    "| 3/20  | Before 11:59 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
