{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "- James Qian: Data curation, Writing\n",
    "- Even Wu: Background research, Writing\n",
    "- Matthew Odom: Conceptualization, Background research, Writing\n",
    "- Aston Martini-Facio: Methodology, Writing\n",
    "\n",
    "list for reference\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question\n",
    "====(in progress)====\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project investigates the relationship between large language model (LLM) performance and token cost. Specifically, it asks: Which publicly available AI models achieve the highest overall benchmark performance, and is that performance proportional to their token costs? To answer this, we will compare models ranked at the top of public leaderboards across multiple evaluation domains (reasoning, coding, math, etc.) with their published pricing structures. We will then fit a predictive model using benchmark scores as independent variables and token cost as the dependent variable, in order to test whether performance can reliably explain or predict pricing.\n",
    "\n",
    "Subquestions might include: \n",
    "\n",
    "- Which models deliver the best overall performance across benchmarks?\n",
    "\n",
    "- Which models provide the most cost-efficient performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models (LLMs) are typically evaluated using standardized benchmarks that measure reasoning, coding, mathematics, and general knowledge. Widely used tasks include MMLU for academic knowledge, GSM8K for multi-step math reasoning, and HumanEval for code generation. These benchmarks provide us with a quantitative basis for comparing model performances, giving us a an accuracy score that can be standarized across different models. Besides static benchmarks, there are also preference-based evaluations such as the LMSYS Chatbot Arena that uses large-scale human comparisons to capture perceived quality across diverse prompts, offering a more holistic view of model performance.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1)\n",
    "\n",
    "Alongside performance data, providers publish detailed token pricing, typically expressed as cost per million input and output tokens. OpenAI, Google and any other providers all differentiate their models by capability tier, context window, and modality, creating a transparent but complex pricing structure.<a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3) While pricing is public, it is not always clear whether higher benchmark performance directly explains higher costs. Some analyses suggest that cost-efficiency varies significantly across models, with smaller or open-source systems sometimes offering better performance-per-dollar on specific tasks.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2)\n",
    "\n",
    "A simple google search reveals this statement insight on 2 top models “ChatGpt is better for general users seeking feature rich experience while deepseek is superior in technical tasks…” (google 2025).  Testing artificial intelligence on a variety of categories can result in metrics which can be used as a foundation for determining what is “best”. Topics which worked in the past to help analyse artificial intelligence are: model, operationalization, proxies, ground truth, bias, machine learning, power, discrimination, agency, privacy and harms, data brokers, consent, goodhearts law, accountability, and standpoint epistemology. Leveraging this vocabulary we can begin to talk about something intangible and circumstantial like “quality” of Ai. Other topics to consider are speed, coding success, accuracy, creativity, multimodality, user experience, and cost. For example if chatGPT is functionally superior, but steals all your data, personal info and even steals your current project that you used chatGPT for, is it “better”?\n",
    "\n",
    "*Past work examples:*\n",
    "\n",
    "In this project<a name=\"cite_ref-5\"></a>[<sup>5</sup>](#cite_note-5) the authors are attempting to experiment with a similar question. “who wins the battle of wits?” chatGPT or deepseek. This author has done something unique, they have made the model itself do most of the actual works. As illustrated by this example, what to name the article. \n",
    "chatGPT : GPT vs. DeepSeek: Which AI Wins for Technical Writing?\n",
    "deepseek: GPT vs Deepseek: Which AI is Your Ultimate Wingman for Technical Writing?\n",
    "-which is better-\n",
    "Through the test making the model to reveal itself by itself doing the test is a clever inception method to run this project. Do we somehow trick the Ai to be tested on testing itself? to devise this test itself may require much planning. A traditional method may result in more clear results.\n",
    "\n",
    "Another example is the ongoing test of the top Ai models in the world of financial trading. started october 17, 2025 the top Ai models are each given 10,000 dollars to practice stock trading. Since then chatGPT has lowered to the bottom group and deepseek has risen to the top.<a name=\"cite_ref-4\"></a>[<sup>4</sup>](#cite_note-4) Since we are seeking information on the quality of the available Ai models on the market today. We will observe the model's token metrics in cross reference with the quality of benchmarks performance tests just like these to determine the result.\n",
    "\n",
    "Prior works have highlighted both the strengths and limitations of current evaluation practices. The Hugging Face Open LLM Leaderboard aggregates multiple benchmarks into composite rankings, displaying trade-offs between accuracy and efficiency.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) Surveys of evaluation methods emphasize the need for multidimensional comparisons, noting that no single benchmark captures the full spectrum of model capabilities.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1)Building on these foundations, our project will investigate whether benchmark performance in terms of accuracy score can reliably predict token pricing.\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1)Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, Ion Stoica. \"LMSYS Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings\", 03 May, 2023. https://lmsys.org/blog/2023-05-03-arena/\n",
    "\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2)Hugging Face Open LLM Leaderboard. \"Leaderboards on the Hub\" https://huggingface.co/docs/leaderboards/en/leaderboards/intro\n",
    "\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3)OpenAI API pricing. https://openai.com/api/pricing\n",
    "\n",
    "4. <a name=\"cite_note-4\"></a> [^](#cite_ref-4)“Ai Trading Benchmark.” Alpha Arena, Accessed 29 Oct. 2025. https://nof1.ai/\n",
    "\n",
    "5. <a name=\"cite_note-5\"></a> [^](#cite_ref-5)“Deepseek vs Chatgpt: Which Ai Is Right for You?” InvoZone, 19 Oct, 2025. https://invozone.com/blog/deepseek-vs-chatgpt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null Hypothesis:** There is no significant relationship between a LLM model’s benchmark accuracy score and its token cost. Improvements in performance do not systematically result in higher or lower token cost.\n",
    "- Benchmark scores and pricing are shaped by different forces. As noted in the background, public pricing is transparent but complex, and smaller or open models can be more cost-efficient on specific tasks—suggesting performance alone may not systematically dictate token cost.\n",
    "\n",
    "**Alternative Hypothesis:** A LLM model's benchmark accuracy is significantly related to token cost — either positively (higher performance requires more cost) or non-linearly (diminishing returns).\n",
    "- Providers often price higher-capability tiers above lower ones, and aggregate benchmarks tend to correlate with perceived quality, which can command premium pricing. However, we also expect non-linearity: gains at the top end may incur disproportionately higher costs due to scaling, yielding diminishing returns.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "%pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>slug</th>\n",
       "      <th>release_date</th>\n",
       "      <th>median_output_tokens_per_second</th>\n",
       "      <th>median_time_to_first_token_seconds</th>\n",
       "      <th>median_time_to_first_answer_token</th>\n",
       "      <th>model_creator.id</th>\n",
       "      <th>model_creator.name</th>\n",
       "      <th>model_creator.slug</th>\n",
       "      <th>...</th>\n",
       "      <th>evaluations.math_500</th>\n",
       "      <th>evaluations.aime</th>\n",
       "      <th>evaluations.aime_25</th>\n",
       "      <th>evaluations.ifbench</th>\n",
       "      <th>evaluations.lcr</th>\n",
       "      <th>evaluations.terminalbench_hard</th>\n",
       "      <th>evaluations.tau2</th>\n",
       "      <th>pricing.price_1m_blended_3_to_1</th>\n",
       "      <th>pricing.price_1m_input_tokens</th>\n",
       "      <th>pricing.price_1m_output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05e45a36-b5c6-47a1-8adb-9ddc19add5b3</td>\n",
       "      <td>GPT-5 nano (minimal)</td>\n",
       "      <td>gpt-5-nano-minimal</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>e67e56e3-15cd-43db-b679-da4660a69f41</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>openai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16149b9c-a1e9-4669-a5cb-ff3c00d78f89</td>\n",
       "      <td>gpt-oss-20B (low)</td>\n",
       "      <td>gpt-oss-20b-low</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>194.081</td>\n",
       "      <td>0.481</td>\n",
       "      <td>10.786</td>\n",
       "      <td>e67e56e3-15cd-43db-b679-da4660a69f41</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>openai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f0083258-8646-45b8-8082-7aaf6c2ea82a</td>\n",
       "      <td>gpt-oss-120B (high)</td>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>364.657</td>\n",
       "      <td>0.385</td>\n",
       "      <td>5.870</td>\n",
       "      <td>e67e56e3-15cd-43db-b679-da4660a69f41</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>openai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c99f3bde-7c08-4de8-bd5c-8ee9123ebffa</td>\n",
       "      <td>gpt-oss-120B (low)</td>\n",
       "      <td>gpt-oss-120b-low</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>337.351</td>\n",
       "      <td>0.374</td>\n",
       "      <td>6.302</td>\n",
       "      <td>e67e56e3-15cd-43db-b679-da4660a69f41</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>openai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36f73aaf-d38a-4b56-a2b3-d04d17186910</td>\n",
       "      <td>gpt-oss-20B (high)</td>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>228.811</td>\n",
       "      <td>0.396</td>\n",
       "      <td>9.136</td>\n",
       "      <td>e67e56e3-15cd-43db-b679-da4660a69f41</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>openai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                  name  \\\n",
       "0  05e45a36-b5c6-47a1-8adb-9ddc19add5b3  GPT-5 nano (minimal)   \n",
       "1  16149b9c-a1e9-4669-a5cb-ff3c00d78f89     gpt-oss-20B (low)   \n",
       "2  f0083258-8646-45b8-8082-7aaf6c2ea82a   gpt-oss-120B (high)   \n",
       "3  c99f3bde-7c08-4de8-bd5c-8ee9123ebffa    gpt-oss-120B (low)   \n",
       "4  36f73aaf-d38a-4b56-a2b3-d04d17186910    gpt-oss-20B (high)   \n",
       "\n",
       "                 slug release_date  median_output_tokens_per_second  \\\n",
       "0  gpt-5-nano-minimal   2025-08-07                            0.000   \n",
       "1     gpt-oss-20b-low   2025-08-05                          194.081   \n",
       "2        gpt-oss-120b   2025-08-05                          364.657   \n",
       "3    gpt-oss-120b-low   2025-08-05                          337.351   \n",
       "4         gpt-oss-20b   2025-08-05                          228.811   \n",
       "\n",
       "   median_time_to_first_token_seconds  median_time_to_first_answer_token  \\\n",
       "0                               0.000                              0.000   \n",
       "1                               0.481                             10.786   \n",
       "2                               0.385                              5.870   \n",
       "3                               0.374                              6.302   \n",
       "4                               0.396                              9.136   \n",
       "\n",
       "                       model_creator.id model_creator.name model_creator.slug  \\\n",
       "0  e67e56e3-15cd-43db-b679-da4660a69f41             OpenAI             openai   \n",
       "1  e67e56e3-15cd-43db-b679-da4660a69f41             OpenAI             openai   \n",
       "2  e67e56e3-15cd-43db-b679-da4660a69f41             OpenAI             openai   \n",
       "3  e67e56e3-15cd-43db-b679-da4660a69f41             OpenAI             openai   \n",
       "4  e67e56e3-15cd-43db-b679-da4660a69f41             OpenAI             openai   \n",
       "\n",
       "   ...  evaluations.math_500  evaluations.aime  evaluations.aime_25  \\\n",
       "0  ...                   NaN               NaN                0.273   \n",
       "1  ...                   NaN               NaN                0.623   \n",
       "2  ...                   NaN               NaN                0.934   \n",
       "3  ...                   NaN               NaN                0.667   \n",
       "4  ...                   NaN               NaN                0.893   \n",
       "\n",
       "   evaluations.ifbench  evaluations.lcr  evaluations.terminalbench_hard  \\\n",
       "0                0.325            0.200                           0.064   \n",
       "1                0.578            0.310                           0.043   \n",
       "2                0.690            0.507                           0.220   \n",
       "3                0.583            0.437                           0.050   \n",
       "4                0.651            0.343                           0.099   \n",
       "\n",
       "   evaluations.tau2  pricing.price_1m_blended_3_to_1  \\\n",
       "0             0.257                            0.138   \n",
       "1             0.503                            0.094   \n",
       "2             0.658                            0.263   \n",
       "3             0.450                            0.263   \n",
       "4             0.602                            0.094   \n",
       "\n",
       "   pricing.price_1m_input_tokens  pricing.price_1m_output_tokens  \n",
       "0                           0.05                             0.4  \n",
       "1                           0.06                             0.2  \n",
       "2                           0.15                             0.6  \n",
       "3                           0.15                             0.6  \n",
       "4                           0.06                             0.2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard = pd.read_csv('data/00-raw/llm_performance_leaderboard.csv')\n",
    "leaderboard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = leaderboard.columns.str.replace('evaluations.', '').str.replace('pricing.','')\n",
    "leaderboard.columns = new_columns\n",
    "leaderboard = leaderboard[['id', 'name', 'release_date', 'model_creator.name',\n",
    "    'artificial_analysis_intelligence_index',\n",
    "    'terminalbench_hard', 'tau2', 'lcr', 'hle', 'mmlu_pro', 'gpqa', \n",
    "    'livecodebench', 'scicode', 'ifbench', 'aime_25',\n",
    "    'price_1m_blended_3_to_1', 'price_1m_input_tokens', 'price_1m_output_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "release_date                                0\n",
       "model_creator.name                          0\n",
       "artificial_analysis_intelligence_index      3\n",
       "terminalbench_hard                        134\n",
       "tau2                                      130\n",
       "lcr                                       121\n",
       "hle                                        35\n",
       "mmlu_pro                                   35\n",
       "gpqa                                       31\n",
       "livecodebench                              36\n",
       "scicode                                    37\n",
       "ifbench                                   121\n",
       "aime_25                                   120\n",
       "price_1m_blended_3_to_1                     0\n",
       "price_1m_input_tokens                       0\n",
       "price_1m_output_tokens                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard = leaderboard.set_index('name').drop(columns='id')\n",
    "leaderboard.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release_date</th>\n",
       "      <th>model_creator.name</th>\n",
       "      <th>artificial_analysis_intelligence_index</th>\n",
       "      <th>terminalbench_hard</th>\n",
       "      <th>tau2</th>\n",
       "      <th>lcr</th>\n",
       "      <th>hle</th>\n",
       "      <th>mmlu_pro</th>\n",
       "      <th>gpqa</th>\n",
       "      <th>livecodebench</th>\n",
       "      <th>scicode</th>\n",
       "      <th>ifbench</th>\n",
       "      <th>aime_25</th>\n",
       "      <th>price_1m_blended_3_to_1</th>\n",
       "      <th>price_1m_input_tokens</th>\n",
       "      <th>price_1m_output_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPT-3.5 Turbo</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>8.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 65B</th>\n",
       "      <td>2023-02-24</td>\n",
       "      <td>Meta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claude Instant</th>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4</th>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>21.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.50</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PALM-2</th>\n",
       "      <td>2023-05-10</td>\n",
       "      <td>Google</td>\n",
       "      <td>6.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)</th>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>26.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Devstral Small (May '25)</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>19.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claude 4 Opus (Non-reasoning)</th>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>42.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.363</td>\n",
       "      <td>30.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o3-pro</th>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>65.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claude 4.1 Opus (Non-reasoning)</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>44.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            release_date model_creator.name  \\\n",
       "name                                                                          \n",
       "GPT-3.5 Turbo                                 2022-11-30             OpenAI   \n",
       "Llama 65B                                     2023-02-24               Meta   \n",
       "Claude Instant                                2023-03-14          Anthropic   \n",
       "GPT-4                                         2023-03-14             OpenAI   \n",
       "PALM-2                                        2023-05-10             Google   \n",
       "...                                                  ...                ...   \n",
       "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)   2025-05-20             NVIDIA   \n",
       "Devstral Small (May '25)                      2025-05-21            Mistral   \n",
       "Claude 4 Opus (Non-reasoning)                 2025-05-22          Anthropic   \n",
       "o3-pro                                        2025-06-10             OpenAI   \n",
       "Claude 4.1 Opus (Non-reasoning)               2025-08-05          Anthropic   \n",
       "\n",
       "                                             artificial_analysis_intelligence_index  \\\n",
       "name                                                                                  \n",
       "GPT-3.5 Turbo                                                                   8.3   \n",
       "Llama 65B                                                                       1.0   \n",
       "Claude Instant                                                                  1.0   \n",
       "GPT-4                                                                          21.5   \n",
       "PALM-2                                                                          6.6   \n",
       "...                                                                             ...   \n",
       "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)                                    26.1   \n",
       "Devstral Small (May '25)                                                       19.6   \n",
       "Claude 4 Opus (Non-reasoning)                                                  42.3   \n",
       "o3-pro                                                                         65.3   \n",
       "Claude 4.1 Opus (Non-reasoning)                                                44.6   \n",
       "\n",
       "                                             terminalbench_hard   tau2   lcr  \\\n",
       "name                                                                           \n",
       "GPT-3.5 Turbo                                               NaN    NaN   NaN   \n",
       "Llama 65B                                                   NaN    NaN   NaN   \n",
       "Claude Instant                                              NaN    NaN   NaN   \n",
       "GPT-4                                                       NaN    NaN   NaN   \n",
       "PALM-2                                                      NaN    NaN   NaN   \n",
       "...                                                         ...    ...   ...   \n",
       "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)                 NaN  0.117  0.00   \n",
       "Devstral Small (May '25)                                    NaN    NaN   NaN   \n",
       "Claude 4 Opus (Non-reasoning)                               NaN    NaN  0.36   \n",
       "o3-pro                                                      NaN    NaN   NaN   \n",
       "Claude 4.1 Opus (Non-reasoning)                             NaN    NaN   NaN   \n",
       "\n",
       "                                               hle  mmlu_pro   gpqa  \\\n",
       "name                                                                  \n",
       "GPT-3.5 Turbo                                  NaN     0.462  0.297   \n",
       "Llama 65B                                      NaN       NaN    NaN   \n",
       "Claude Instant                               0.038     0.434  0.330   \n",
       "GPT-4                                          NaN       NaN    NaN   \n",
       "PALM-2                                         NaN       NaN    NaN   \n",
       "...                                            ...       ...    ...   \n",
       "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)  0.051     0.556  0.408   \n",
       "Devstral Small (May '25)                     0.040     0.632  0.434   \n",
       "Claude 4 Opus (Non-reasoning)                0.059     0.860  0.701   \n",
       "o3-pro                                         NaN       NaN  0.845   \n",
       "Claude 4.1 Opus (Non-reasoning)                NaN       NaN    NaN   \n",
       "\n",
       "                                             livecodebench  scicode  ifbench  \\\n",
       "name                                                                           \n",
       "GPT-3.5 Turbo                                          NaN      NaN      NaN   \n",
       "Llama 65B                                              NaN      NaN      NaN   \n",
       "Claude Instant                                       0.109      NaN      NaN   \n",
       "GPT-4                                                  NaN      NaN      NaN   \n",
       "PALM-2                                                 NaN      NaN      NaN   \n",
       "...                                                    ...      ...      ...   \n",
       "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)          0.493    0.101    0.255   \n",
       "Devstral Small (May '25)                             0.258    0.245      NaN   \n",
       "Claude 4 Opus (Non-reasoning)                        0.542    0.409    0.433   \n",
       "o3-pro                                                 NaN      NaN      NaN   \n",
       "Claude 4.1 Opus (Non-reasoning)                        NaN      NaN      NaN   \n",
       "\n",
       "                                             aime_25  price_1m_blended_3_to_1  \\\n",
       "name                                                                            \n",
       "GPT-3.5 Turbo                                    NaN                     0.75   \n",
       "Llama 65B                                        NaN                     0.00   \n",
       "Claude Instant                                   NaN                     0.00   \n",
       "GPT-4                                            NaN                    37.50   \n",
       "PALM-2                                           NaN                     0.00   \n",
       "...                                              ...                      ...   \n",
       "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)    0.500                     0.00   \n",
       "Devstral Small (May '25)                         NaN                     0.15   \n",
       "Claude 4 Opus (Non-reasoning)                  0.363                    30.00   \n",
       "o3-pro                                           NaN                    35.00   \n",
       "Claude 4.1 Opus (Non-reasoning)                  NaN                    30.00   \n",
       "\n",
       "                                             price_1m_input_tokens  \\\n",
       "name                                                                 \n",
       "GPT-3.5 Turbo                                                  0.5   \n",
       "Llama 65B                                                      0.0   \n",
       "Claude Instant                                                 0.0   \n",
       "GPT-4                                                         30.0   \n",
       "PALM-2                                                         0.0   \n",
       "...                                                            ...   \n",
       "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)                    0.0   \n",
       "Devstral Small (May '25)                                       0.1   \n",
       "Claude 4 Opus (Non-reasoning)                                 15.0   \n",
       "o3-pro                                                        20.0   \n",
       "Claude 4.1 Opus (Non-reasoning)                               15.0   \n",
       "\n",
       "                                             price_1m_output_tokens  \n",
       "name                                                                 \n",
       "GPT-3.5 Turbo                                                   1.5  \n",
       "Llama 65B                                                       0.0  \n",
       "Claude Instant                                                  0.0  \n",
       "GPT-4                                                          60.0  \n",
       "PALM-2                                                          0.0  \n",
       "...                                                             ...  \n",
       "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)                     0.0  \n",
       "Devstral Small (May '25)                                        0.3  \n",
       "Claude 4 Opus (Non-reasoning)                                  75.0  \n",
       "o3-pro                                                         80.0  \n",
       "Claude 4.1 Opus (Non-reasoning)                                75.0  \n",
       "\n",
       "[140 rows x 16 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard[leaderboard.isnull().any(axis=1)].sort_values(by='release_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OpenAI', 'xAI', 'Meta', 'Google', 'Anthropic', 'Mistral',\n",
       "       'DeepSeek', 'Perplexity', 'Amazon', 'Microsoft Azure', 'Liquid AI',\n",
       "       'Upstage', 'MiniMax', 'NVIDIA', 'Moonshot AI', 'IBM', 'Reka AI',\n",
       "       'Nous Research', 'LG AI Research', 'Baidu', 'Z AI', 'Cohere',\n",
       "       'ServiceNow', 'AI21 Labs', 'Alibaba', 'InclusionAI',\n",
       "       'ByteDance Seed', 'OpenChat', 'Databricks',\n",
       "       'Allen Institute for AI', 'Snowflake', '01.AI'], dtype=object)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard['model_creator.name'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "> The data we are using is primarily anonymous, and although using LLMs and training them involves user data, in this instance in relation to our question it doesn’t expose any personally identifiable information. \n",
    "\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    "> Yes they are. We accurately display performance comparisons between AI models without manipulating scales or selectively reporting results.\n",
    "\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "> Yes, all data collection methods, prompts, model settings, and evaluation criteria for AI models are clearly recorded. This ensures that if any issues arise in the future, the experiments can be accurately replicated and verified for consistency.\n",
    "\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "> Yes, we have taken steps to ensure that neither model’s evaluation relies on variables or proxies that could be unfairly discriminatory. Our comparisons focus only on objective performance metrics such as accuracy, response relevance, and coherence rather than user demographics or sensitive attributes.\n",
    "\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    "> Yes, we carefully considered the effects of optimizing for our chosen metrics. While we primarily used metrics such as accuracy, response quality, and coherence were prioritized, we also reviewed other factors like bias, response diversity, and factual consistency\n",
    "\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    "> Yes, both models’ decisions and outputs can be explained in understandable terms when justification is needed. For example, we analyze responses by looking at the prompt structure, model training objectives, and language generation patterns to clarify why each model produced a specific answer, and taking This approach helps interpret differences in reasoning, tone, or factual accuracy between different AI models.\n",
    "\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use discord for communication, expected response time: 1hr, briefly meet up after lectures\n",
    "2. be polite and respectful\n",
    "3. decisions are made collectively\n",
    "4. specialization in tasks\n",
    "5. plan ahead, try to not have last minute works as it greatly increase the chance of something to go wrong\n",
    "6. if someone doesn't do their part the rest of the team will need to complete the unfinished work in time to turn it in. the person who has errored may be reported."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal\n",
    "====(in progress)===="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 11/4  |  2 PM | Find relevant datasets  | Discuss which datasets to use, and work load distribution| \n",
    "| 11/11  |  2 PM |  have a first draft of the section responsible | Discuss details to edit before submission | \n",
    "| 11/18  | 2 PM  | EDA; Begin Analysis | Discuss possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 11/25  | 2 PM  | TBD |  TBD |\n",
    "| 12/2  | 2 PM  | TBD |TBD |\n",
    "| 12/9  | 2 PM  |TBD | TBD |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
