{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "- James Qian: Data curation, Writing, Analysis, Background research\n",
    "- Even Wu: Background research, Writing, Analysis, Data curation,\n",
    "- Aston Martini-Facio: Methodology, Writing, Background research\n",
    "- Matthew Odom (**droped from the class**): Conceptualization, Background research, Writing\n",
    "\n",
    "list for reference\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question\n",
    "\n",
    "**Can benchmark performance across different domains (reasoning, coding, mathematics, and general knowledge) reliably predict the token pricing of AI language models?**\n",
    "\n",
    "Subquestions might include: \n",
    "\n",
    "- Which models deliver the best overall performance across benchmarks?\n",
    "\n",
    "- Which models provide the most cost-efficient performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models (LLMs) are largely evaluated using standardized benchmarks to measure performance in reasoning, coding, mathematics, and general knowledge. For example, some widely used tasks include MMLU for academic knowledge, GSM8K for multi-step math reasoning, and HumanEval for code generation. These benchmarks give each model an overall accuracy score that can be compared across multiple systems. In addition to these static tests, preference-based evaluations such as the LMSYS Chatbot Arena use large-scale human comparisons to capture perceived quality across diverse prompts, offering a much broader and holistic view of model performance.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1)\n",
    "\n",
    "Alongside performance data, providers also publish detailed token pricing, usually expressed as cost per million input and output tokens. OpenAI, Google, and other companies differentiate their models by capability tier, context window, and modality, which results in a pricing structure that is viewable to the general public, however interpreting and analyzing this data is much harder.<a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3) While pricing is public, it is not obvious whether higher benchmark scores actually justify higher token costs. Some analyses suggest that cost-efficiency varies widely across models, and that smaller or open-source systems can sometimes deliver better “performance per dollar” on specific tasks compared to their larger counterparts.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2)\n",
    "\n",
    "The findings of this project will be able to help both students and researchers. Students in college span a wide variety of majors, and in todays age, they now mostly all use LLMs in their lives. However, each students’ use for LLMs differs and hence finding the best model for their task is critical, for example: computer science students may care most about coding accuracy, math and statistics students about problem-solving, and social science or humanities students about writing quality and general reasoning. Furthermore, many students also work within a limited budget. If one model is cheaper but nearly as accurate as a flagship model for a given domain, it may be the better practical choice. Moreover, understanding whether or not newer models are actually becoming more cost-efficient and which models are best suited to particular types of tasks could help students pick tools that fit both their needs and their budgets.\n",
    "\n",
    "*Past work examples:*\n",
    "\n",
    "Past work has begun to explore model comparisons in more applied settings. For example, articles and blog posts can be found directly comparing “ChatGPT vs DeepSeek,” noting that one system may feel better for general use while another excels on more technical tasks.<a name=\"cite_ref-5\"></a>[<sup>5</sup>](#cite_note-5) There are even ongoing experiments that let different AI systems trade a virtual portfolio to compare their performance in financial markets over time.<a name=\"cite_ref-4\"></a>[<sup>4</sup>](#cite_note-4) These examples highlight that “best” depends heavily on the task at hand.\n",
    "\n",
    "\n",
    "\n",
    "Prior work on LLM evaluation has shown both the strengths and limits of current approaches. For example, the Hugging Face Open LLM Leaderboard combines results from many benchmarks into a single view, which helps make trade-offs between accuracy, efficiency, and model size easier to see.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) Other reviews of evaluation methods argue that models should be judged along several dimensions, since no single benchmark can capture everything an LLM can do.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1) Building on this, our project will compare large language models ranked on public leaderboards across multiple evaluation domains with their published pricing structures. We will then fit a predictive model using benchmark scores as independent variables and token cost as the dependent variable, in order to test whether performance can reliably explain or predict pricing.\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1)Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, Ion Stoica. \"LMSYS Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings\", 03 May, 2023. https://lmsys.org/blog/2023-05-03-arena/\n",
    "\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2)Hugging Face Open LLM Leaderboard. \"Leaderboards on the Hub\" https://huggingface.co/docs/leaderboards/en/leaderboards/intro\n",
    "\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3)OpenAI API pricing. https://openai.com/api/pricing\n",
    "\n",
    "4. <a name=\"cite_note-4\"></a> [^](#cite_ref-4)“Ai Trading Benchmark.” Alpha Arena, Accessed 29 Oct. 2025. https://nof1.ai/\n",
    "\n",
    "5. <a name=\"cite_note-5\"></a> [^](#cite_ref-5)“Deepseek vs Chatgpt: Which Ai Is Right for You?” InvoZone, 19 Oct, 2025. https://invozone.com/blog/deepseek-vs-chatgpt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null Hypothesis:** There is no significant relationship between a LLM model’s benchmark accuracy score and its token cost. Improvements in performance do not systematically result in higher or lower token cost.\n",
    "\n",
    "**Alternative Hypothesis:** A LLM model's benchmark accuracy is significantly related to token cost — either positively (higher performance requires more cost) or non-linearly (diminishing returns).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "%pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "\n",
    "leaderboard_path = 'data/00-raw/llm_performance_leaderboard.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LLM Performance Leaderboard Dataset**\n",
    "\n",
    "## **Dataset Column Descriptions:**\n",
    "---\n",
    "### **Basic Model Information**\n",
    "- **id** – Unique numerical identifier for each model.  \n",
    "- **name** – Human-readable name of the model (e.g., “GPT-5-mini”).  \n",
    "- **slug** – URL-friendly identifier for the model.  \n",
    "- **release_date** – Date the model was released.\n",
    "\n",
    "### **Speed & Latency Metrics**\n",
    "- **median_output_tokens_per_second** – Model generation speed in tokens per second (higher = faster).  \n",
    "- **median_time_to_first_token_seconds** – Time until the first generated token appears (seconds).  \n",
    "- **median_time_to_first_answer_token** – Time until the model begins producing its final answer.\n",
    "\n",
    "### **Creator Metadata**\n",
    "- **model_creator.id** – Identifier for the organization that developed the model.  \n",
    "- **model_creator.name** – Name of the model’s creator (e.g., OpenAI, Anthropic).  \n",
    "- **model_creator.slug** – URL-friendly version of the creator name.\n",
    "\n",
    "---\n",
    "### **Performance Metrics (Evaluation Scores)**  \n",
    "All scores represent benchmark accuracy or composite abilities; higher values indicate better performance.\n",
    "\n",
    "### **Composite Scores**\n",
    "- **evaluations.artificial_analysis_intelligence_index** – Overall composite reasoning score.  \n",
    "- **evaluations.artificial_analysis_coding_index** – Composite coding/programming ability score.  \n",
    "- **evaluations.artificial_analysis_math_index** – Composite mathematical reasoning score.\n",
    "\n",
    "### **Academic / Professional Benchmarks**\n",
    "- **evaluations.mmlu_pro** – Accuracy on the professional-level MMLU-Pro exam.  \n",
    "- **evaluations.gpqa** – Accuracy on the graduate-level GPQA benchmark.  \n",
    "- **evaluations.hle** – High-Level Evaluation score for advanced reasoning.  \n",
    "- **evaluations.livecodebench** – Coding performance on real-time coding tasks.  \n",
    "- **evaluations.scicode** – Scientific-reasoning benchmark accuracy.  \n",
    "- **evaluations.math_500** – Accuracy on 500 diverse math problems.  \n",
    "- **evaluations.aime** – Score on AIME (American Invitational Math Exam).  \n",
    "- **evaluations.aime_25** – Score on the 2025 AIME-style problem set.  \n",
    "- **evaluations.ifbench** – Logical inference (IFBench) reasoning accuracy.  \n",
    "- **evaluations.lcr** – LCR reasoning benchmark score.  \n",
    "- **evaluations.terminalbench_hard** – Performance on hard terminal-based coding tasks.  \n",
    "- **evaluations.tau2** – Score on the TAU2 advanced reasoning benchmark.\n",
    "---\n",
    "\n",
    "### **Pricing Metrics**  \n",
    "Prices are in **USD per 1,000,000 tokens**.\n",
    "\n",
    "- **pricing.price_1m_blended_3_to_1** – Estimated cost assuming a 3:1 input-to-output token ratio.  \n",
    "- **pricing.price_1m_input_tokens** – Cost per one million input (prompt) tokens.  \n",
    "- **pricing.price_1m_output_tokens** – Cost per one million output (generated) tokens.\n",
    "---\n",
    "### **Major Concerns**\n",
    "A major concern with this dataset is that many benchmark scores are missing for a large number of models. Several evaluation suites—such as AIME, IFBench, LCR, TAU2, and TerminalBench—only test a small subset of high-end or well-known models, meaning mid-tier models appear to have missing values not because they perform poorly, but simply because they were never evaluated. This selective inclusion introduces bias and makes direct comparison across all models difficult. Another issue is that different benchmarks are run by different organizations using slightly different prompting setups, evaluation conditions, or decoding parameters, which reduces cross-benchmark consistency. Additionally, token pricing is influenced by company strategy and business decisions rather than purely by model performance, so price should not be interpreted as an objective reflection of capability. Finally, the dataset only includes models that are publicly listed and evaluated through certain platforms, which excludes many open-source or privately deployed models and may skew the sample toward commercial systems from major AI labs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset:\n",
    "import pandas as pd\n",
    "leaderboard = pd.read_csv('data/00-raw/llm_performance_leaderboard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>slug</th>\n",
       "      <th>release_date</th>\n",
       "      <th>median_output_tokens_per_second</th>\n",
       "      <th>median_time_to_first_token_seconds</th>\n",
       "      <th>median_time_to_first_answer_token</th>\n",
       "      <th>model_creator.id</th>\n",
       "      <th>model_creator.name</th>\n",
       "      <th>model_creator.slug</th>\n",
       "      <th>...</th>\n",
       "      <th>evaluations.math_500</th>\n",
       "      <th>evaluations.aime</th>\n",
       "      <th>evaluations.aime_25</th>\n",
       "      <th>evaluations.ifbench</th>\n",
       "      <th>evaluations.lcr</th>\n",
       "      <th>evaluations.terminalbench_hard</th>\n",
       "      <th>evaluations.tau2</th>\n",
       "      <th>pricing.price_1m_blended_3_to_1</th>\n",
       "      <th>pricing.price_1m_input_tokens</th>\n",
       "      <th>pricing.price_1m_output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05e45a36-b5c6-47a1-8adb-9ddc19add5b3</td>\n",
       "      <td>GPT-5 nano (minimal)</td>\n",
       "      <td>gpt-5-nano-minimal</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>e67e56e3-15cd-43db-b679-da4660a69f41</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>openai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16149b9c-a1e9-4669-a5cb-ff3c00d78f89</td>\n",
       "      <td>gpt-oss-20B (low)</td>\n",
       "      <td>gpt-oss-20b-low</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>194.081</td>\n",
       "      <td>0.481</td>\n",
       "      <td>10.786</td>\n",
       "      <td>e67e56e3-15cd-43db-b679-da4660a69f41</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>openai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f0083258-8646-45b8-8082-7aaf6c2ea82a</td>\n",
       "      <td>gpt-oss-120B (high)</td>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>364.657</td>\n",
       "      <td>0.385</td>\n",
       "      <td>5.870</td>\n",
       "      <td>e67e56e3-15cd-43db-b679-da4660a69f41</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>openai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c99f3bde-7c08-4de8-bd5c-8ee9123ebffa</td>\n",
       "      <td>gpt-oss-120B (low)</td>\n",
       "      <td>gpt-oss-120b-low</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>337.351</td>\n",
       "      <td>0.374</td>\n",
       "      <td>6.302</td>\n",
       "      <td>e67e56e3-15cd-43db-b679-da4660a69f41</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>openai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36f73aaf-d38a-4b56-a2b3-d04d17186910</td>\n",
       "      <td>gpt-oss-20B (high)</td>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>228.811</td>\n",
       "      <td>0.396</td>\n",
       "      <td>9.136</td>\n",
       "      <td>e67e56e3-15cd-43db-b679-da4660a69f41</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>openai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                  name  \\\n",
       "0  05e45a36-b5c6-47a1-8adb-9ddc19add5b3  GPT-5 nano (minimal)   \n",
       "1  16149b9c-a1e9-4669-a5cb-ff3c00d78f89     gpt-oss-20B (low)   \n",
       "2  f0083258-8646-45b8-8082-7aaf6c2ea82a   gpt-oss-120B (high)   \n",
       "3  c99f3bde-7c08-4de8-bd5c-8ee9123ebffa    gpt-oss-120B (low)   \n",
       "4  36f73aaf-d38a-4b56-a2b3-d04d17186910    gpt-oss-20B (high)   \n",
       "\n",
       "                 slug release_date  median_output_tokens_per_second  \\\n",
       "0  gpt-5-nano-minimal   2025-08-07                            0.000   \n",
       "1     gpt-oss-20b-low   2025-08-05                          194.081   \n",
       "2        gpt-oss-120b   2025-08-05                          364.657   \n",
       "3    gpt-oss-120b-low   2025-08-05                          337.351   \n",
       "4         gpt-oss-20b   2025-08-05                          228.811   \n",
       "\n",
       "   median_time_to_first_token_seconds  median_time_to_first_answer_token  \\\n",
       "0                               0.000                              0.000   \n",
       "1                               0.481                             10.786   \n",
       "2                               0.385                              5.870   \n",
       "3                               0.374                              6.302   \n",
       "4                               0.396                              9.136   \n",
       "\n",
       "                       model_creator.id model_creator.name model_creator.slug  \\\n",
       "0  e67e56e3-15cd-43db-b679-da4660a69f41             OpenAI             openai   \n",
       "1  e67e56e3-15cd-43db-b679-da4660a69f41             OpenAI             openai   \n",
       "2  e67e56e3-15cd-43db-b679-da4660a69f41             OpenAI             openai   \n",
       "3  e67e56e3-15cd-43db-b679-da4660a69f41             OpenAI             openai   \n",
       "4  e67e56e3-15cd-43db-b679-da4660a69f41             OpenAI             openai   \n",
       "\n",
       "   ...  evaluations.math_500  evaluations.aime  evaluations.aime_25  \\\n",
       "0  ...                   NaN               NaN                0.273   \n",
       "1  ...                   NaN               NaN                0.623   \n",
       "2  ...                   NaN               NaN                0.934   \n",
       "3  ...                   NaN               NaN                0.667   \n",
       "4  ...                   NaN               NaN                0.893   \n",
       "\n",
       "   evaluations.ifbench  evaluations.lcr  evaluations.terminalbench_hard  \\\n",
       "0                0.325            0.200                           0.064   \n",
       "1                0.578            0.310                           0.043   \n",
       "2                0.690            0.507                           0.220   \n",
       "3                0.583            0.437                           0.050   \n",
       "4                0.651            0.343                           0.099   \n",
       "\n",
       "   evaluations.tau2  pricing.price_1m_blended_3_to_1  \\\n",
       "0             0.257                            0.138   \n",
       "1             0.503                            0.094   \n",
       "2             0.658                            0.263   \n",
       "3             0.450                            0.263   \n",
       "4             0.602                            0.094   \n",
       "\n",
       "   pricing.price_1m_input_tokens  pricing.price_1m_output_tokens  \n",
       "0                           0.05                             0.4  \n",
       "1                           0.06                             0.2  \n",
       "2                           0.15                             0.6  \n",
       "3                           0.15                             0.6  \n",
       "4                           0.06                             0.2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first few rows of the dataset:\n",
    "leaderboard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'name',\n",
       " 'slug',\n",
       " 'release_date',\n",
       " 'median_output_tokens_per_second',\n",
       " 'median_time_to_first_token_seconds',\n",
       " 'median_time_to_first_answer_token',\n",
       " 'model_creator.id',\n",
       " 'model_creator.name',\n",
       " 'model_creator.slug',\n",
       " 'evaluations.artificial_analysis_intelligence_index',\n",
       " 'evaluations.artificial_analysis_coding_index',\n",
       " 'evaluations.artificial_analysis_math_index',\n",
       " 'evaluations.mmlu_pro',\n",
       " 'evaluations.gpqa',\n",
       " 'evaluations.hle',\n",
       " 'evaluations.livecodebench',\n",
       " 'evaluations.scicode',\n",
       " 'evaluations.math_500',\n",
       " 'evaluations.aime',\n",
       " 'evaluations.aime_25',\n",
       " 'evaluations.ifbench',\n",
       " 'evaluations.lcr',\n",
       " 'evaluations.terminalbench_hard',\n",
       " 'evaluations.tau2',\n",
       " 'pricing.price_1m_blended_3_to_1',\n",
       " 'pricing.price_1m_input_tokens',\n",
       " 'pricing.price_1m_output_tokens']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the columns:\n",
    "leaderboard.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>release_date</th>\n",
       "      <th>model_creator.name</th>\n",
       "      <th>artificial_analysis_intelligence_index</th>\n",
       "      <th>terminalbench_hard</th>\n",
       "      <th>tau2</th>\n",
       "      <th>lcr</th>\n",
       "      <th>hle</th>\n",
       "      <th>mmlu_pro</th>\n",
       "      <th>gpqa</th>\n",
       "      <th>livecodebench</th>\n",
       "      <th>scicode</th>\n",
       "      <th>ifbench</th>\n",
       "      <th>aime_25</th>\n",
       "      <th>price_1m_blended_3_to_1</th>\n",
       "      <th>price_1m_input_tokens</th>\n",
       "      <th>price_1m_output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05e45a36-b5c6-47a1-8adb-9ddc19add5b3</td>\n",
       "      <td>GPT-5 nano (minimal)</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>29.1</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16149b9c-a1e9-4669-a5cb-ff3c00d78f89</td>\n",
       "      <td>gpt-oss-20B (low)</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>44.3</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f0083258-8646-45b8-8082-7aaf6c2ea82a</td>\n",
       "      <td>gpt-oss-120B (high)</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c99f3bde-7c08-4de8-bd5c-8ee9123ebffa</td>\n",
       "      <td>gpt-oss-120B (low)</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>47.5</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36f73aaf-d38a-4b56-a2b3-d04d17186910</td>\n",
       "      <td>gpt-oss-20B (high)</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>52.4</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                  name release_date  \\\n",
       "0  05e45a36-b5c6-47a1-8adb-9ddc19add5b3  GPT-5 nano (minimal)   2025-08-07   \n",
       "1  16149b9c-a1e9-4669-a5cb-ff3c00d78f89     gpt-oss-20B (low)   2025-08-05   \n",
       "2  f0083258-8646-45b8-8082-7aaf6c2ea82a   gpt-oss-120B (high)   2025-08-05   \n",
       "3  c99f3bde-7c08-4de8-bd5c-8ee9123ebffa    gpt-oss-120B (low)   2025-08-05   \n",
       "4  36f73aaf-d38a-4b56-a2b3-d04d17186910    gpt-oss-20B (high)   2025-08-05   \n",
       "\n",
       "  model_creator.name  artificial_analysis_intelligence_index  \\\n",
       "0             OpenAI                                    29.1   \n",
       "1             OpenAI                                    44.3   \n",
       "2             OpenAI                                    60.5   \n",
       "3             OpenAI                                    47.5   \n",
       "4             OpenAI                                    52.4   \n",
       "\n",
       "   terminalbench_hard   tau2    lcr    hle  mmlu_pro   gpqa  livecodebench  \\\n",
       "0               0.064  0.257  0.200  0.041     0.556  0.428          0.470   \n",
       "1               0.043  0.503  0.310  0.051     0.718  0.611          0.652   \n",
       "2               0.220  0.658  0.507  0.185     0.808  0.782          0.878   \n",
       "3               0.050  0.450  0.437  0.052     0.775  0.672          0.707   \n",
       "4               0.099  0.602  0.343  0.098     0.748  0.688          0.777   \n",
       "\n",
       "   scicode  ifbench  aime_25  price_1m_blended_3_to_1  price_1m_input_tokens  \\\n",
       "0    0.291    0.325    0.273                    0.138                   0.05   \n",
       "1    0.340    0.578    0.623                    0.094                   0.06   \n",
       "2    0.389    0.690    0.934                    0.263                   0.15   \n",
       "3    0.360    0.583    0.667                    0.263                   0.15   \n",
       "4    0.344    0.651    0.893                    0.094                   0.06   \n",
       "\n",
       "   price_1m_output_tokens  \n",
       "0                     0.4  \n",
       "1                     0.2  \n",
       "2                     0.6  \n",
       "3                     0.6  \n",
       "4                     0.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove columns that are not relavent to our research & renaming some columns:\n",
    "leaderboard.columns = leaderboard.columns.str.replace('evaluations.', '').str.replace('pricing.','')\n",
    "leaderboard = leaderboard[['id', 'name', 'release_date', 'model_creator.name', 'artificial_analysis_intelligence_index',\n",
    "    'terminalbench_hard', 'tau2', 'lcr', 'hle', 'mmlu_pro', 'gpqa', 'livecodebench', 'scicode', 'ifbench', 'aime_25',\n",
    "    'price_1m_blended_3_to_1', 'price_1m_input_tokens', 'price_1m_output_tokens']]\n",
    "\n",
    "leaderboard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether 'name' is a unique identifier for each row\n",
    "len(leaderboard['name'].unique()) == leaderboard.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release_date</th>\n",
       "      <th>model_creator.name</th>\n",
       "      <th>artificial_analysis_intelligence_index</th>\n",
       "      <th>terminalbench_hard</th>\n",
       "      <th>tau2</th>\n",
       "      <th>lcr</th>\n",
       "      <th>hle</th>\n",
       "      <th>mmlu_pro</th>\n",
       "      <th>gpqa</th>\n",
       "      <th>livecodebench</th>\n",
       "      <th>scicode</th>\n",
       "      <th>ifbench</th>\n",
       "      <th>aime_25</th>\n",
       "      <th>price_1m_blended_3_to_1</th>\n",
       "      <th>price_1m_input_tokens</th>\n",
       "      <th>price_1m_output_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPT-5 nano (minimal)</th>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>29.1</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-oss-20B (low)</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>44.3</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-oss-120B (high)</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-oss-120B (low)</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>47.5</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-oss-20B (high)</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>52.4</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     release_date model_creator.name  \\\n",
       "name                                                   \n",
       "GPT-5 nano (minimal)   2025-08-07             OpenAI   \n",
       "gpt-oss-20B (low)      2025-08-05             OpenAI   \n",
       "gpt-oss-120B (high)    2025-08-05             OpenAI   \n",
       "gpt-oss-120B (low)     2025-08-05             OpenAI   \n",
       "gpt-oss-20B (high)     2025-08-05             OpenAI   \n",
       "\n",
       "                      artificial_analysis_intelligence_index  \\\n",
       "name                                                           \n",
       "GPT-5 nano (minimal)                                    29.1   \n",
       "gpt-oss-20B (low)                                       44.3   \n",
       "gpt-oss-120B (high)                                     60.5   \n",
       "gpt-oss-120B (low)                                      47.5   \n",
       "gpt-oss-20B (high)                                      52.4   \n",
       "\n",
       "                      terminalbench_hard   tau2    lcr    hle  mmlu_pro  \\\n",
       "name                                                                      \n",
       "GPT-5 nano (minimal)               0.064  0.257  0.200  0.041     0.556   \n",
       "gpt-oss-20B (low)                  0.043  0.503  0.310  0.051     0.718   \n",
       "gpt-oss-120B (high)                0.220  0.658  0.507  0.185     0.808   \n",
       "gpt-oss-120B (low)                 0.050  0.450  0.437  0.052     0.775   \n",
       "gpt-oss-20B (high)                 0.099  0.602  0.343  0.098     0.748   \n",
       "\n",
       "                       gpqa  livecodebench  scicode  ifbench  aime_25  \\\n",
       "name                                                                    \n",
       "GPT-5 nano (minimal)  0.428          0.470    0.291    0.325    0.273   \n",
       "gpt-oss-20B (low)     0.611          0.652    0.340    0.578    0.623   \n",
       "gpt-oss-120B (high)   0.782          0.878    0.389    0.690    0.934   \n",
       "gpt-oss-120B (low)    0.672          0.707    0.360    0.583    0.667   \n",
       "gpt-oss-20B (high)    0.688          0.777    0.344    0.651    0.893   \n",
       "\n",
       "                      price_1m_blended_3_to_1  price_1m_input_tokens  \\\n",
       "name                                                                   \n",
       "GPT-5 nano (minimal)                    0.138                   0.05   \n",
       "gpt-oss-20B (low)                       0.094                   0.06   \n",
       "gpt-oss-120B (high)                     0.263                   0.15   \n",
       "gpt-oss-120B (low)                      0.263                   0.15   \n",
       "gpt-oss-20B (high)                      0.094                   0.06   \n",
       "\n",
       "                      price_1m_output_tokens  \n",
       "name                                          \n",
       "GPT-5 nano (minimal)                     0.4  \n",
       "gpt-oss-20B (low)                        0.2  \n",
       "gpt-oss-120B (high)                      0.6  \n",
       "gpt-oss-120B (low)                       0.6  \n",
       "gpt-oss-20B (high)                       0.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since 'name' is an unique identifier for each row, drop the 'id' column and set 'name' as the index:\n",
    "leaderboard = leaderboard.set_index('name').drop(columns='id')\n",
    "leaderboard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "release_date                                0\n",
       "model_creator.name                          0\n",
       "artificial_analysis_intelligence_index      3\n",
       "terminalbench_hard                        134\n",
       "tau2                                      130\n",
       "lcr                                       121\n",
       "hle                                        35\n",
       "mmlu_pro                                   35\n",
       "gpqa                                       31\n",
       "livecodebench                              36\n",
       "scicode                                    37\n",
       "ifbench                                   121\n",
       "aime_25                                   120\n",
       "price_1m_blended_3_to_1                     0\n",
       "price_1m_input_tokens                       0\n",
       "price_1m_output_tokens                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out the null values each column have:\n",
    "leaderboard.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that some benchmarks have significantly more null entries than others, we decided to focus on these specific benchmarks that have comparatively less null values:\n",
    "- hle\n",
    "- mmlu_pro\n",
    "- gpqa\n",
    "- livecodebench\n",
    "- scicode\n",
    "\n",
    "since these benchmarks are all testing on similar topics (Reasoning, Coding, Knowledge), droping the other columns does not compromise the overall coverage of the analysis, as the retained benchmarks still capture the core competencies being evaluated. At the same time, it allow us to keep more data after droping rows containing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the other benchmark columns:\n",
    "leaderboard = leaderboard.drop(columns=['terminalbench_hard', 'tau2', 'lcr', 'ifbench', 'aime_25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows containing any null values:\n",
    "leaderboard = leaderboard.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "release_date                              False\n",
       "model_creator.name                        False\n",
       "artificial_analysis_intelligence_index    False\n",
       "hle                                       False\n",
       "mmlu_pro                                  False\n",
       "gpqa                                      False\n",
       "livecodebench                             False\n",
       "scicode                                   False\n",
       "price_1m_blended_3_to_1                   False\n",
       "price_1m_input_tokens                     False\n",
       "price_1m_output_tokens                    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "release_date                               object\n",
       "model_creator.name                         object\n",
       "artificial_analysis_intelligence_index    float64\n",
       "hle                                       float64\n",
       "mmlu_pro                                  float64\n",
       "gpqa                                      float64\n",
       "livecodebench                             float64\n",
       "scicode                                   float64\n",
       "price_1m_blended_3_to_1                   float64\n",
       "price_1m_input_tokens                     float64\n",
       "price_1m_output_tokens                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all numerical variables are float type\n",
    "leaderboard.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artificial_analysis_intelligence_index</th>\n",
       "      <th>hle</th>\n",
       "      <th>mmlu_pro</th>\n",
       "      <th>gpqa</th>\n",
       "      <th>livecodebench</th>\n",
       "      <th>scicode</th>\n",
       "      <th>price_1m_blended_3_to_1</th>\n",
       "      <th>price_1m_input_tokens</th>\n",
       "      <th>price_1m_output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>281.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>281.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.051601</td>\n",
       "      <td>0.066274</td>\n",
       "      <td>0.666107</td>\n",
       "      <td>0.538922</td>\n",
       "      <td>0.394730</td>\n",
       "      <td>0.251416</td>\n",
       "      <td>1.622495</td>\n",
       "      <td>0.868089</td>\n",
       "      <td>3.868950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.495261</td>\n",
       "      <td>0.041654</td>\n",
       "      <td>0.179972</td>\n",
       "      <td>0.185807</td>\n",
       "      <td>0.242261</td>\n",
       "      <td>0.118668</td>\n",
       "      <td>4.070670</td>\n",
       "      <td>2.104984</td>\n",
       "      <td>10.045099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.400000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>29.700000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.800000</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>1.313000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>2.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>68.500000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       artificial_analysis_intelligence_index         hle    mmlu_pro  \\\n",
       "count                              281.000000  281.000000  281.000000   \n",
       "mean                                31.051601    0.066274    0.666107   \n",
       "std                                 16.495261    0.041654    0.179972   \n",
       "min                                  1.000000    0.028000    0.055000   \n",
       "25%                                 17.400000    0.043000    0.571000   \n",
       "50%                                 29.700000    0.050000    0.730000   \n",
       "75%                                 43.800000    0.072000    0.805000   \n",
       "max                                 68.500000    0.265000    0.880000   \n",
       "\n",
       "             gpqa  livecodebench     scicode  price_1m_blended_3_to_1  \\\n",
       "count  281.000000     281.000000  281.000000               281.000000   \n",
       "mean     0.538922       0.394730    0.251416                 1.622495   \n",
       "std      0.185807       0.242261    0.118668                 4.070670   \n",
       "min      0.098000       0.002000    0.000000                 0.000000   \n",
       "25%      0.371000       0.181000    0.156000                 0.085000   \n",
       "50%      0.557000       0.343000    0.267000                 0.419000   \n",
       "75%      0.698000       0.617000    0.354000                 1.313000   \n",
       "max      0.877000       0.878000    0.465000                30.000000   \n",
       "\n",
       "       price_1m_input_tokens  price_1m_output_tokens  \n",
       "count             281.000000              281.000000  \n",
       "mean                0.868089                3.868950  \n",
       "std                 2.104984               10.045099  \n",
       "min                 0.000000                0.000000  \n",
       "25%                 0.050000                0.150000  \n",
       "50%                 0.200000                0.800000  \n",
       "75%                 0.700000                2.800000  \n",
       "max                15.000000               75.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary statistics:\n",
    "leaderboard.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release_date</th>\n",
       "      <th>model_creator.name</th>\n",
       "      <th>artificial_analysis_intelligence_index</th>\n",
       "      <th>hle</th>\n",
       "      <th>mmlu_pro</th>\n",
       "      <th>gpqa</th>\n",
       "      <th>livecodebench</th>\n",
       "      <th>scicode</th>\n",
       "      <th>price_1m_blended_3_to_1</th>\n",
       "      <th>price_1m_input_tokens</th>\n",
       "      <th>price_1m_output_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPT-5 nano (minimal)</th>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>29.1</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-oss-20B (low)</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>44.3</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-oss-120B (high)</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-oss-120B (low)</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>47.5</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-oss-20B (high)</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>52.4</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5 Instruct 32B</th>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>Alibaba</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3 235B A22B (Reasoning)</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>Alibaba</td>\n",
       "      <td>41.7</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.399</td>\n",
       "      <td>2.625</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3 0.6B (Reasoning)</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>Alibaba</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3 Max (Preview)</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>Alibaba</td>\n",
       "      <td>48.5</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.370</td>\n",
       "      <td>2.400</td>\n",
       "      <td>1.20</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yi-Large</th>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>01.AI</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            release_date model_creator.name  \\\n",
       "name                                                          \n",
       "GPT-5 nano (minimal)          2025-08-07             OpenAI   \n",
       "gpt-oss-20B (low)             2025-08-05             OpenAI   \n",
       "gpt-oss-120B (high)           2025-08-05             OpenAI   \n",
       "gpt-oss-120B (low)            2025-08-05             OpenAI   \n",
       "gpt-oss-20B (high)            2025-08-05             OpenAI   \n",
       "...                                  ...                ...   \n",
       "Qwen2.5 Instruct 32B          2024-09-19            Alibaba   \n",
       "Qwen3 235B A22B (Reasoning)   2025-04-28            Alibaba   \n",
       "Qwen3 0.6B (Reasoning)        2025-04-28            Alibaba   \n",
       "Qwen3 Max (Preview)           2025-09-05            Alibaba   \n",
       "Yi-Large                      2024-05-13              01.AI   \n",
       "\n",
       "                             artificial_analysis_intelligence_index    hle  \\\n",
       "name                                                                         \n",
       "GPT-5 nano (minimal)                                           29.1  0.041   \n",
       "gpt-oss-20B (low)                                              44.3  0.051   \n",
       "gpt-oss-120B (high)                                            60.5  0.185   \n",
       "gpt-oss-120B (low)                                             47.5  0.052   \n",
       "gpt-oss-20B (high)                                             52.4  0.098   \n",
       "...                                                             ...    ...   \n",
       "Qwen2.5 Instruct 32B                                           22.9  0.038   \n",
       "Qwen3 235B A22B (Reasoning)                                    41.7  0.117   \n",
       "Qwen3 0.6B (Reasoning)                                         14.2  0.057   \n",
       "Qwen3 Max (Preview)                                            48.5  0.093   \n",
       "Yi-Large                                                       13.4  0.032   \n",
       "\n",
       "                             mmlu_pro   gpqa  livecodebench  scicode  \\\n",
       "name                                                                   \n",
       "GPT-5 nano (minimal)            0.556  0.428          0.470    0.291   \n",
       "gpt-oss-20B (low)               0.718  0.611          0.652    0.340   \n",
       "gpt-oss-120B (high)             0.808  0.782          0.878    0.389   \n",
       "gpt-oss-120B (low)              0.775  0.672          0.707    0.360   \n",
       "gpt-oss-20B (high)              0.748  0.688          0.777    0.344   \n",
       "...                               ...    ...            ...      ...   \n",
       "Qwen2.5 Instruct 32B            0.697  0.466          0.248    0.229   \n",
       "Qwen3 235B A22B (Reasoning)     0.828  0.700          0.622    0.399   \n",
       "Qwen3 0.6B (Reasoning)          0.347  0.239          0.121    0.028   \n",
       "Qwen3 Max (Preview)             0.838  0.764          0.651    0.370   \n",
       "Yi-Large                        0.586  0.363          0.108    0.188   \n",
       "\n",
       "                             price_1m_blended_3_to_1  price_1m_input_tokens  \\\n",
       "name                                                                          \n",
       "GPT-5 nano (minimal)                           0.138                   0.05   \n",
       "gpt-oss-20B (low)                              0.094                   0.06   \n",
       "gpt-oss-120B (high)                            0.263                   0.15   \n",
       "gpt-oss-120B (low)                             0.263                   0.15   \n",
       "gpt-oss-20B (high)                             0.094                   0.06   \n",
       "...                                              ...                    ...   \n",
       "Qwen2.5 Instruct 32B                           0.000                   0.00   \n",
       "Qwen3 235B A22B (Reasoning)                    2.625                   0.70   \n",
       "Qwen3 0.6B (Reasoning)                         0.398                   0.11   \n",
       "Qwen3 Max (Preview)                            2.400                   1.20   \n",
       "Yi-Large                                       0.000                   0.00   \n",
       "\n",
       "                             price_1m_output_tokens  \n",
       "name                                                 \n",
       "GPT-5 nano (minimal)                           0.40  \n",
       "gpt-oss-20B (low)                              0.20  \n",
       "gpt-oss-120B (high)                            0.60  \n",
       "gpt-oss-120B (low)                             0.60  \n",
       "gpt-oss-20B (high)                             0.20  \n",
       "...                                             ...  \n",
       "Qwen2.5 Instruct 32B                           0.00  \n",
       "Qwen3 235B A22B (Reasoning)                    8.40  \n",
       "Qwen3 0.6B (Reasoning)                         1.26  \n",
       "Qwen3 Max (Preview)                            6.00  \n",
       "Yi-Large                                       0.00  \n",
       "\n",
       "[281 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data is now **clean** and **tidy**:\n",
    "- Each variable forms a column\n",
    "- Each observation forms a row\n",
    "- There are no missing values\n",
    "- Unit is standardized\n",
    "- Irrelevant columns are droped\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "> The data we are using is primarily anonymous, and although using LLMs and training them involves user data, in this instance in relation to our question it doesn’t expose any personally identifiable information. \n",
    "\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    "> Yes they are. We accurately display performance comparisons between AI models without manipulating scales or selectively reporting results.\n",
    "\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "> Yes, all data collection methods, prompts, model settings, and evaluation criteria for AI models are clearly recorded. This ensures that if any issues arise in the future, the experiments can be accurately replicated and verified for consistency.\n",
    "\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "> Yes, we have taken steps to ensure that neither model’s evaluation relies on variables or proxies that could be unfairly discriminatory. Our comparisons focus only on objective performance metrics such as accuracy, response relevance, and coherence rather than user demographics or sensitive attributes.\n",
    "\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    "> Yes, we carefully considered the effects of optimizing for our chosen metrics. While we primarily used metrics such as accuracy, response quality, and coherence were prioritized, we also reviewed other factors like bias, response diversity, and factual consistency\n",
    "\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    "> Yes, both models’ decisions and outputs can be explained in understandable terms when justification is needed. For example, we analyze responses by looking at the prompt structure, model training objectives, and language generation patterns to clarify why each model produced a specific answer, and taking This approach helps interpret differences in reasoning, tone, or factual accuracy between different AI models.\n",
    "\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use discord for communication, expected response time: 1hr, briefly meet up after lectures\n",
    "2. be polite and respectful\n",
    "3. decisions are made collectively\n",
    "4. specialization in tasks\n",
    "5. plan ahead, try to not have last minute works as it greatly increase the chance of something to go wrong\n",
    "6. if someone doesn't do their part the rest of the team will need to complete the unfinished work in time to turn it in. the person who has errored may be reported."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 11/4  |  2 PM | Searched for large public datasets on LLM performance and pricing; reviewed project prompt and brainstormed research ideas.  | Met to choose the Hugging Face Open LLM Leaderboard + provider pricing pages as our main data sources and discussed the direction of our project.| \n",
    "| 11/11  |  2 PM |  Downloaded and uploaded the leaderboard data, did first-round of data cleaning (selecting variables, renaming columns, checking missingness), and re-drafting proposal sections. | Met to merge and edit the proposal based on feedback, and confirm that the cleaned dataset fits our research question, and submit second data check point. | \n",
    "| 11/18  | 2 PM  | EDA; Begin Analysis. Build a processed dataset with one row per model and run exploratory data analysis (summary stats, basic plots of performance and token cost). | Meet to review EDA and visualizations, decide how to construct composite performance scores, and agree on the main modeling approach.   |\n",
    "| 11/25  | 2 PM  | Finalize checkpoint 02. Run baseline models (correlation and simple regressions of performance vs. token cost, including domain-specific analyses) and create initial draft plots. | Discuss details to edit before submission. Short check-in meeting to interpret early results, decide which relationships to focus on and possible correlations, and adjust the analysis plan if needed. |\n",
    "| 12/2  | 2 PM  | Finalize the full set of analyses and produce polished figures and tables showing the relationship between benchmark performance and token pricing. Start drafting up conclusion and findings of study. | Discuss feedback from TA. Meeting focused on outlining the final report sections (Intro, Methods, Results, Discussion, Conclusion).|\n",
    "| 12/9  | 2 PM  | Complete full drafts of all report sections and ensure code + visualizations are fully reproducible from the cleaned dataset. | Finalize project, format checking. Meet to do group edits, tighten the narrative so it clearly answers the research question, finalize formatting and citations, and submit the final project|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
